<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aleira N. Sanchez - Labs</title>
    <link rel="stylesheet" href="style.css">
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
</head>
<body>
    <!-- Navigation Bar -->
    <nav>
        <a href="index.html" class="logo">Aleira N. Sanchez</a>
        <ul>
            <!-- Other sections -->
            <li class="dropdown">
                <span class="bold-text">Labs</span>
                <div class="dropdown-content">
                    <a href="lab1.html">Lab 1: Artemis and Bluetooth</a>
                    <a href="lab2.html">Lab 2: IMU</a>
                    <a href="lab3.html">Lab 3: ToF</a>
                    <a href="lab4.html">Lab 4: Motor Drivers and Open Loop Control</a>
                    <a href="lab5.html">Lab 5: Linear PID and Linear Interpolation</a>
                    <a href="lab6.html">Lab 6: Orientation PID</a>
                    <a href="lab7.html">Lab 7: Kalman Filtering</a>
                    <a href="lab8.html">Lab 8: Stunts!</a>
                    <a href="lab9.html">Lab 9: Mapping</a>
                    <a href="lab10.html">Lab 10: Localization (sim)</a>
                    <a href="lab11.html">Lab 11: Localization (real)</a>
                    <a href="lab12.html">Lab 12: Planning and Execution (real)</a>
                    <!-- Add more labs as needed -->
                </div>
            </li>
        </ul>
    </nav>

    <button class="home-button" onclick="goHome()">Return to Home</button>

    <script>
        function goHome() {
            window.location.href = "index.html"; // Adjust if your homepage is in a different folder
        }
    </script>

    
    <!-- Labs Details Section -->
    <section id="labs-subpage">
        <div class="gallery">
            <section id="lab11">
                <div class="lab-text">
                    <h1>Lab 11: Localization(real)</h1>
                    <h2>TASKS: </h2>
                    <h3>LOCALIZATION IN SIM:</h3>
                    <p>To start the lab, our first task was to run the Bayes Filter simulation provided in the lab11_sim.ipynb file. After running the simulation and moving the virtual robot around the given space, I obtained the plot shown below. In the plot: red represents odometry, green represents ground truth, and blue represents belief.</p>
                    <img src="Lab11Task1.png" alt="sim localization" class="lab-image">
                  
                    <h3>ARDUINO IDE CODE:</h3>
                    <p>In order to continue with the rest of the tasks for Lab 11, I needed my Lab 9 code. Since I had not completed Lab 9, I started by working on that. I wrote code that would make the robot perform a 360-degree turn in place, recording distance sensor measurements every 20 degrees.</p>
                    <p>Here is my Lab 9 code. First is the ORIENTATION_CONTROL command and then there is the BLE connected while loop:</p>
                    <p><a href="orient360turnCase.png" target="_blank">ORIENTATION_CONTROL Command</a></p>
                    <p><a href="BLEconnected loop.png" target="_blank">BLE While Loop</a></p>
                    <p>And here is a video of my robot doing the 360 turn:</p>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/1QNpkMAeTVc?si=XvfJxGKYaSzzPzvK" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

                    <h3>POLAR PLOTS:</h3>
                    <p>After reverting my yaw values to be in radians and tweaking my time of flight sensor code because I was getting junky data. I went ahead and ran my orientation control twice for each of the 4 coordinates in the lab set up. At times the left wheels would stop working and they would get jammed which caused the robot to move from the original coordinate - this effect can be seen mostly in the data for Run 1.</p>
                    <table>
                      <tr>
                        <td><img src="polar for top left 0,3.png" alt="polar TL" width="300"></td>
                        <td><img src="polar for top right 5,3.png" alt="polar TR" width="300"></td>
                      </tr>
                      <tr>
                        <td><img src="polar for bottom left -3,-2.png" alt="polar BL" width="300"></td>
                        <td><img src="polar for bottom right 5,-3.png" alt="polar BR" width="300"></td>
                      </tr>
                    </table>
                    <h3>GLOBAL FRAME PLOTS:</h3>
                    <p>I then converted the polar measurements, relative to the car, to the inertial reference frame of the lab room. I used the transformation matrix:</p>
                    <p>
                    \[
                    \begin{bmatrix}
                    x \\ y
                    \end{bmatrix}
                    =
                    r
                    \begin{bmatrix}
                    \cos(\theta) \\ \sin(\theta)
                    \end{bmatrix}
                    \]
                    </p>
                    <p>I plotted all the four coordinates' first run in the same plot. I also added the lab set up's walls to the plot to observe how precise tho ToF sensor actually is. The effect of the robot moving out of its original spot can be seen mostly on the readings from the top right coordinate (5,3).</p>
                    <img src="line global run 1.png" alt="global frame run 1" class="lab-image">
                    <img src="line global run 2.png" alt="global frame run 2" class="lab-image">

<!--                     <pre><code>
                    </code></pre> -->

                    <h3>PERFORM_OBSERVATION_LOOP:</h3>
                    <p>The perform_observation_loop function is designed to collect 18 observations during a full 360-degree rotation and store them in the sensor_ranges and sensor_bearings arrays. These arrays will later be used in the update step to populate the obs_range_data member variable. In my function, I first initialize the arrays that will later hold the distance and angle measurements. Then, I call my command cases to stop orientation control and send the collected data to the computer. I continue calling the asyncio function until 18 readings are collected (corresponding to a full rotation), and then I stop the notification handler. In the final steps of the function, I iterate over the collected readings and convert the lists into NumPy column arrays.</p>
                    <img src="perform_observation_loop.png" alt="perform_oservation_loop function" class="lab-image">
                    <p>This function is then called in the "Run an update step of the Bayes Filter" section of the lab11_real.ipynb notebook. However, before running that cell I call a few commands and initialize my arrays that will collect my data.</p>
                    <img src="initial command cases called.png" alt="command cases" class="lab-image">
                    
                    <h3>RUN ROBOT LOCALIZATION:</h3>
                    <p>LAST TIME: For the next task, I had to run my code twice for each of the poses marked on the lab setup. These poses are: (-3ft, -2ft, 0 deg), (0ft, 3ft, 0 deg), (5ft, -3ft, 0 deg), and (5ft, 3ft, 0 deg). I ran the code 3-4 times for each pose and I got the same two coordinates for all of them. I tried changing initial orientation at each pose and did 0, 90, 180 and -90 degrees for each pose and at the bottom left pose I even did like an angle between -90 and 180 to see if its any better and still no improvement. The sensor_readings and sensor_ranges arrays both have pretty accurate values for distances and yaw measurements however the coordinates for the belief and the ground truth were always one of these two:</p>
                    <img src="topRight_run1_data.png" alt="top right run 1" class="lab-image">
                    <img src="topRight_run2_data.png" alt="top right run 2" class="lab-image">
                    <img src="bottom right_run2_data.png" alt="bottom right run 2" class="lab-image">
                    <p>LAST TIME: First two pictures are runs at different orientations of the same pose (5ft, 3ft) and the second one is the second run of the bottom right pose, (5ft, - 3ft,) since the first run gave me the exact same values while in a different initial orientation. My results for the left points, both upper and lower, alternated between those two coordinates no matter the orientation I tried. At once I tried going clockwise instead of counter-clockwise but that made my robot spin nonstop so I returned it to counter-clockwise.</p>
                    <p>LAST TIME: Here are the belief (blue) and ground truth(green) plots of the 1st run on the top right pose,(5ft, 3ft). </p>
                    <table>
                        <tr>
                            <td><img src="topRight_run1_belief.png" alt="top right belief 1" width="300"></td>
                            <td><img src="topRight_run1_GT.png" alt="top right ground truth 1" width="300"></td>
                        </tr>
                    </table>
                    <p>RECENT UPDATE: I was able to improve my data. My ToF sensor measurements were not accurate so I altered my code a bit at the part where I save the distance measurements and I was able to get better measurements. With these I got the following plots for the belief:</p>
                    <table>
                      <tr>
                        <td><img src="top left.png" alt="belief TL" width="300"></td>
                        <td><img src="top right?.png" alt="belief TR" width="300"></td>
                      </tr>
                      <tr>
                        <td><img src="bottom left?.png" alt="belief BL" width="300"></td>
                        <td><img src="bottom right?.png" alt="belief BR" width="300"></td>
                      </tr>
                    </table>
                    
                    
                    <h3>REFERENCES:</h3>
                    <p>For this lab I referenced, Stephan Wagner's and Mikayla Lahr's labs from previous years. I also used ChatGPT to help with the grammar.</p>
            </section>
        </div>
    </section>
</body>
</html>
